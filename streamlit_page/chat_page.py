# chat_page.py
def chat_page():
    import sys
    import os
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

    import warnings
    import streamlit as st
    from dotenv import load_dotenv
    from langchain.prompts import PromptTemplate
    from langchain_openai import ChatOpenAI
    from langchain.callbacks.base import BaseCallbackHandler
    from streamlit_chat import message

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ ì •ì˜
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    class StreamHandler(BaseCallbackHandler):
        def __init__(self, container):
            self.container = container
            self.output = ""

        def on_llm_new_token(self, token: str, **kwargs) -> None:
            self.output += token
            self.container.markdown(self.output)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    load_dotenv()
    warnings.filterwarnings("ignore", category=UserWarning)

    api_key = os.getenv("OPENAI_API_KEY")
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, api_key=api_key, streaming=True)

    prompt_template = PromptTemplate(
        input_variables=["keyword"],
        template="""'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•´ ì‚¬ìš©ìê°€ ChatGPTì— ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì§ˆë¬¸ 5ê°œë¥¼ ë§Œë“¤ì–´ì¤˜. 
    ê° ë¬¸ì¥ì€ 1ì¤„ë¡œ ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì¤˜.
    ì¶œë ¥ í˜•ì‹ì€ ë²ˆí˜¸ ì—†ì´, ì§ˆë¬¸ 5ê°œë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë‚˜ì—´í•´ì¤˜."""
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.session_state.keywords = [
        "ê°€ì", "ì´ìŠ¤ë¼ì—˜", "ê¸°ì•„", "ì˜ì–‘ì‹¤ì¡°", "ë³´ê±´",
        "ìœ ë‹ˆì„¸í”„", "ì ì‹­ì", "ì„ì‹ ë¶€", "ë§Œì„±ì§ˆí™˜", "ì¸ë„ì–‘ë¶€ì§€êµ¬íŠ¹ë³„ìœ„ì›íšŒ"
    ]

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    if "generated_prompts" not in st.session_state:
        st.session_state.generated_prompts = []

    if "pending_prompt_question" not in st.session_state:
        st.session_state.pending_prompt_question = None

    if "trigger_from_prompt" not in st.session_state:
        st.session_state.trigger_from_prompt = False

    if "user_profile" not in st.session_state:
        st.session_state.user_profile = {
            "ì„±ë³„": "",
            "ë‚˜ì´": "",
            "í‚¤": "",
            "ëª¸ë¬´ê²Œ": ""
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. ì‚¬ìš©ì ê±´ê°• ì •ë³´ ì…ë ¥
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ‘¤ ê±´ê°• ì •ë³´ ì…ë ¥ (ì„ íƒ)"):
        gender = st.selectbox("ì„±ë³„", ["", "ë‚¨ì„±", "ì—¬ì„±"])
        age = st.text_input("ë‚˜ì´ (ìˆ«ìë§Œ)")
        height = st.text_input("í‚¤ (cm)")
        weight = st.text_input("ëª¸ë¬´ê²Œ (kg)")

        st.session_state.user_profile = {
            "ì„±ë³„": gender,
            "ë‚˜ì´": age,
            "í‚¤": height,
            "ëª¸ë¬´ê²Œ": weight
        }

    def get_health_context():
        info = st.session_state.user_profile
        return f"ì‚¬ìš©ì ì •ë³´: ì„±ë³„={info['ì„±ë³„']}, ë‚˜ì´={info['ë‚˜ì´']}ì„¸, í‚¤={info['í‚¤']}cm, ëª¸ë¬´ê²Œ={info['ëª¸ë¬´ê²Œ']}kg"

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5. UI ë° ì§ˆë¬¸ íë¦„
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.title("í‚¤ì›Œë“œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ & ê±´ê°• ì±—ë´‡")

    # í‚¤ì›Œë“œë³„ ë²„íŠ¼ ìƒì„±
    if st.session_state.keywords:
        st.markdown("### ğŸ” í‚¤ì›Œë“œë¥¼ í´ë¦­í•´ ì§ˆë¬¸ ì˜ˆì‹œë¥¼ ìƒì„±í•˜ì„¸ìš”")
        kw_cols = st.columns(4)
        for i, kw in enumerate(st.session_state.keywords):
            if kw_cols[i % 4].button(kw):
                chain = prompt_template | llm
                with st.spinner(f"'{kw}' í‚¤ì›Œë“œë¡œ ì§ˆë¬¸ ì˜ˆì‹œ ìƒì„± ì¤‘..."):
                    response = chain.invoke({"keyword": kw})
                    prompts = response.content.strip().split("\n")
                    st.session_state.generated_prompts = [p.strip() for p in prompts if p.strip()]

    # í”„ë¡¬í”„íŠ¸ â†’ ì§ˆë¬¸ ì‹¤í–‰
    prompts = st.session_state.get("generated_prompts", [])
    if prompts:
        st.markdown("### ğŸ’¡ í”„ë¡¬í”„íŠ¸ë¥¼ í´ë¦­í•˜ë©´ ì•„ë˜ì—ì„œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆì–´ìš”")
        cols = st.columns(2)
        for idx, prompt in enumerate(prompts):
            if cols[idx % 2].button(prompt):
                st.session_state.pending_prompt_question = prompt
                st.session_state.trigger_from_prompt = True
                st.rerun()

    # ì§ì ‘ ì§ˆë¬¸ UI
    st.markdown("### âœï¸ ì§ì ‘ ì§ˆë¬¸í•˜ê¸°")
    user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”", key="direct_input")

    if st.button("ë‹µë³€ ë°›ê¸°", key="direct_submit") and user_input.strip():
        context = get_health_context()
        full_prompt = context + "\nì§ˆë¬¸: " + user_input.strip()

        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            placeholder = st.empty()
            handler = StreamHandler(placeholder)

            llm_with_stream = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0.7,
                api_key=api_key,
                streaming=True,
                callbacks=[handler],
            )

            response = llm_with_stream.invoke(full_prompt)
            st.session_state.chat_history.append((user_input.strip(), response.content))

    # í”„ë¡¬í”„íŠ¸ ì§ˆë¬¸ â†’ ì•„ë˜ì—ì„œ ë‹µë³€ ì‹¤í–‰
    if st.session_state.trigger_from_prompt and st.session_state.pending_prompt_question:
        context = get_health_context()
        prompt_question = st.session_state.pending_prompt_question
        full_prompt = context + "\nì§ˆë¬¸: " + prompt_question

        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            placeholder = st.empty()
            handler = StreamHandler(placeholder)

            llm_with_stream = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0.7,
                api_key=api_key,
                streaming=True,
                callbacks=[handler],
            )

            response = llm_with_stream.invoke(full_prompt)
            st.session_state.chat_history.append((prompt_question, response.content))

        st.session_state.trigger_from_prompt = False
        st.session_state.pending_prompt_question = None

    # ëŒ€í™” ê¸°ë¡
    if st.session_state.chat_history:
        st.markdown("---")
        st.subheader("ğŸ’¬ ëŒ€í™” ê¸°ë¡")
        for idx, (q, a) in enumerate(st.session_state.chat_history):
            message(q, is_user=True, key=f"user_{idx}")
            message(a, is_user=False, key=f"bot_{idx}")